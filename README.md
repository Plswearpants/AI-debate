# AI Debate Platform

An AI-driven debating platform that simulates high-fidelity, text-based argumentation between two agents on complex social topics.

## ğŸ¯ Project Status

**Current Phase**: Week 2 - Foundation + Debator + Cost Controls Complete âœ…  
**Tests Passing**: 97/97 âœ…  
**Code Quality**: Production-ready, all design decisions implemented âœ…  
**API Keys Required**: Not yet (using mocks) â³

---

## ğŸ—ï¸ Architecture

The platform features a sophisticated multi-agent system:

### Agent Ecosystem (ALL COMPLETE! ğŸ‰)
- **debator_a & debator_b**: Generate arguments (Gemini Deep Research Agent) âœ…
- **factchecker_a & factchecker_b**: Verify citations (Perplexity Sonar Pro) âœ…
- **judge**: Neutral analysis and frontier mapping (Claude 3.5 Sonnet) âœ…
- **crowd**: 100 diverse personas voting (Llama 3.1 8B on Lambda GPU) âœ…
- **moderator**: Orchestration engine (Python state machine) â³ NEXT

### Key Features
- **Adversarial Verification**: Fact-checkers scrutinize opponent's sources
- **Latent Space Mapping**: Judge identifies disagreement frontier
- **Dynamic Resource Allocation**: Adjusts for audience bias
- **Permission-Based Data Access**: Each agent sees only what they should

---

## ğŸ“‚ Project Structure

```
AI-debate/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/          # Agent implementations
â”‚   â”‚   â””â”€â”€ base.py      # Abstract base class âœ…
â”‚   â”œâ”€â”€ clients/         # LLM API wrappers
â”‚   â”‚   â”œâ”€â”€ gemini_client.py      âœ…
â”‚   â”‚   â”œâ”€â”€ claude_client.py      âœ…
â”‚   â”‚   â”œâ”€â”€ perplexity_client.py  âœ…
â”‚   â”‚   â”œâ”€â”€ lambda_client.py      âœ…
â”‚   â”‚   â””â”€â”€ mcp_client.py         âœ…
â”‚   â”œâ”€â”€ utils/           # Core utilities
â”‚   â”‚   â”œâ”€â”€ file_manager.py    âœ…
â”‚   â”‚   â””â”€â”€ state_manager.py   âœ…
â”‚   â”œâ”€â”€ prompts/         # System prompts
â”‚   â”œâ”€â”€ outputs/         # Output generators
â”‚   â””â”€â”€ config.py        # Configuration âœ…
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/           # Unit tests (83 tests) âœ…
â”‚   â”œâ”€â”€ integration/    # Integration tests
â”‚   â””â”€â”€ e2e/           # End-to-end tests
â”œâ”€â”€ debates/           # Debate instances
â””â”€â”€ docs/              # Documentation
    â”œâ”€â”€ MVP.md         # Product specification
    â”œâ”€â”€ ARCHITECTURE.md # Technical design
    â”œâ”€â”€ ROADMAP.md     # Implementation plan
    â”œâ”€â”€ TEST_PLAN.md   # Testing strategy
    â”œâ”€â”€ MODEL_CONFIG.md # Model allocation
    â””â”€â”€ PROGRESS.md    # Development progress
```

---

## ğŸš€ Getting Started

### Prerequisites
- Python 3.11+
- pip

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/AI-debate.git
cd AI-debate
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure environment** (when ready for real testing)
```bash
cp env.example .env
# Edit .env with your API keys
```

### Running Tests

```bash
# Run all unit tests
pytest tests/unit/ -v

# Run specific test file
pytest tests/unit/test_file_manager.py -v

# Run with coverage
pytest tests/unit/ --cov=src --cov-report=html
```

Current test status: **83/83 passing** âœ…

---

## ğŸ”‘ API Keys (Not Yet Required)

You'll need these API keys when testing with real models:

1. **Gemini API** (for debators)
   - Get from: https://makersuite.google.com/app/apikey
   - Model: gemini-1.5-pro
   - Cost: ~$0.15 per debate

2. **Claude API** (for judge)
   - Get from: https://console.anthropic.com/
   - Model: claude-3-5-sonnet-20241022
   - Cost: ~$0.25 per debate

3. **Perplexity API** (for factcheckers)
   - Get from: https://www.perplexity.ai/settings/api
   - Model: sonar-pro
   - Cost: ~$0.30 per debate

4. **Lambda GPU** (for crowd)
   - Setup: Deploy Llama 3.1 8B with vLLM
   - Cost: ~$0.10 per debate (with $400 credit)

**Total cost per debate: ~$0.80**

---

## ğŸ“Š Development Roadmap

- [x] **Week 1**: Foundation (File Manager, State Manager, Config) âœ…
- [x] **Week 2**: LLM Clients + Debator Agent + Cost Controls âœ…
- [ ] **Week 3**: Agent Implementation (FactChecker, Judge, Crowd)
- [ ] **Week 4**: Orchestration (Moderator workflow)
- [ ] **Week 5**: Output Generation (Transcript, Ledger, Map, CSV)
- [ ] **Week 6**: Integration Testing
- [ ] **Week 7**: Production Deployment

---

## ğŸ§ª Testing Strategy

### Current Coverage
- **Unit Tests**: 97 tests covering foundation + debator âœ…
- **Integration Tests**: Coming in Week 3
- **End-to-End Tests**: Coming in Week 4

### New Features Tested
- âœ… Deep Research integration
- âœ… Structured output parsing
- âœ… Cost control system
- âœ… Budget tracking and fallbacks

### Test Philosophy
- Build with mocks first (no API keys needed)
- Test architecture before testing AI quality
- Fast tests (< 5 seconds for all unit tests)
- High coverage (aim for 80%+)

---

## ğŸ“– Documentation

- **MVP.md**: Product specification and requirements
- **ARCHITECTURE.md**: Technical design and interfaces
- **ROADMAP.md**: 7-week implementation plan
- **TEST_PLAN.md**: Comprehensive testing strategy
- **MODEL_CONFIG.md**: Model allocation and cost breakdown
- **PROGRESS.md**: Current development status

---

## ğŸ“ How It Works

### Debate Flow

1. **Phase 0: Initialization**
   - Crowd votes on stance (Vote Zero)
   - Winner becomes Team a
   - Resource multiplier calculated if bias > 60%

2. **Phase 1: Opening**
   - debator_a: Research + generate opening
   - factchecker_b: Verify a's citations
   - debator_b: Research + generate opening
   - factchecker_a: Verify b's citations
   - judge: Analyze and identify frontier
   - crowd: Vote (Vote 1)

3. **Phase 2: Debate Rounds** (default: 2)
   - factchecker_a: Defense + scrutinize b's citations
   - debator_a: Generate rebuttal targeting frontier
   - factchecker_b: Defense + scrutinize a's citations
   - debator_b: Generate rebuttal targeting frontier
   - judge: Update frontier
   - crowd: Vote

4. **Phase 3: Closing**
   - Final fact-checking
   - Closing statements (no new citations)
   - Final judge report
   - Final crowd vote

### Output Artifacts
- `transcript_full.md`: Human-readable debate transcript
- `citation_ledger.json`: All citations with verification scores
- `debate_logic_map.json`: Evolution of disagreement frontier
- `voter_sentiment_graph.csv`: Crowd opinion shifts over time

---

## ğŸ¤ Contributing

This is currently in active development. See PROGRESS.md for current status.

---

## ğŸ“ License

TBD

---

## ğŸ‘¤ Author

Dong Chen - January 2026

---

## ğŸ”— Related Documentation

- See [MVP.md](MVP.md) for detailed product specification
- See [ARCHITECTURE.md](ARCHITECTURE.md) for technical design
- See [PROGRESS.md](PROGRESS.md) for current development status
